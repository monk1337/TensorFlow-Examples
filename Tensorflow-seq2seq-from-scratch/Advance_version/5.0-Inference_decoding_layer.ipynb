{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference_decoding_layer.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "yg_sDgBNQtMG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3X7QqMqGcFlZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#during inference we will feed ith time step to i+1th as input so we will not use traininghelper we will use greedyembeddinghelper\n",
        "#which require embedding matrix as input\n",
        "#outout will be embedding vectors of batch size \n",
        "\n",
        "symbols_dict = {'<PAD>':0,'<GO>':1,'<EOS>':2}\n",
        "\n",
        "max_size = 5\n",
        "batch_size=6\n",
        "vocab_size=32\n",
        "hidden_dim = 10\n",
        "\n",
        "\n",
        "#start token\n",
        "\n",
        "start_token = tf.tile(tf.constant([symbols_dict['<GO>']],dtype=tf.int32),[batch_size])\n",
        "\n",
        "\n",
        "#end_token\n",
        "end_token= symbols_dict['<EOS>']\n",
        "\n",
        "#embedding_matrix\n",
        "\n",
        "embedding_matrix = tf.get_variable(name='embedding_matrix',\n",
        "                                   shape=[vocab_size,hidden_dim],\n",
        "                                   initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M2LBEKMZdWCG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#greedyembedding_helper\n",
        "\n",
        "greedy_embedding = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_matrix,start_token,end_token)\n",
        "\n",
        "#output of greedy_embedding for each timestep will be batch x embedding_dim\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6dzHFlIXdo0K",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Basic_decoder\n",
        "\n",
        "#lstm cell\n",
        "cell= tf.contrib.rnn.LSTMCell(15)\n",
        "\n",
        "#zero_state\n",
        "initial_state= cell.zero_state(dtype=tf.float32,batch_size=batch_size)\n",
        "\n",
        "#output_layer\n",
        "output = tf.layers.Dense(vocab_size)\n",
        "\n",
        "#decoder\n",
        "\n",
        "decoder_ = tf.contrib.seq2seq.BasicDecoder(cell,greedy_embedding,initial_state,output_layer=output)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7cCdu0HTek4c",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46b04166-1f3e-489a-eb22-372e9a823466",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530660915093,
          "user_tz": -330,
          "elapsed": 933,
          "user": {
            "displayName": "ayodhyankit paul",
            "photoUrl": "//lh3.googleusercontent.com/-aLSMOExWjxQ/AAAAAAAAAAI/AAAAAAAAAAc/yPMgEhPgnpk/s50-c-k-no/photo.jpg",
            "userId": "106815194044651409765"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#dynamic_decode\n",
        "\n",
        "dynamic_decode = tf.contrib.seq2seq.dynamic_decode(decoder_,impute_finished=False,maximum_iterations=5)\n",
        "\n",
        "#output will be  rnn_output , lstm_tuple , time_finished\n",
        "\n",
        "#logits shape will be  batch_size x max_time x vocab_size\n",
        "\n",
        "\n",
        "#rnn_output will be logits , sample_id (argamx over logits)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  logits_,lstm_tuple , batch_size = sess.run(dynamic_decode)\n",
        "  \n",
        "  output , sample_id = logits_\n",
        "  \n",
        "  print(output.shape)\n",
        "  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 5, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jz_7-wnSezhI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
