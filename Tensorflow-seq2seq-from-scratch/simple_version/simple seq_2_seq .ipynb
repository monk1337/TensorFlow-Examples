{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T17:48:08.764203Z",
     "start_time": "2018-06-30T17:48:08.757767Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "#reference : https://arxiv.org/pdf/1409.3215.pdf\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T17:48:09.805443Z",
     "start_time": "2018-06-30T17:48:09.797017Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some hyper parameters\n",
    "\n",
    "encoder_vocab_size = 100  # suppose our encoding data vocab size is 100 \n",
    "decoder_vocab_size = 150  # and decoding data vocab size is 150\n",
    "\n",
    "batch_size=10\n",
    "\n",
    "encoder_hidden_unit = 100\n",
    "decoder_hidden_unit = 100\n",
    "\n",
    "encoder_embedding_dim=50\n",
    "decoder_embedding_dim=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T17:48:10.975173Z",
     "start_time": "2018-06-30T17:48:10.961838Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"encoder_input:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#inputs :  encoder_input , decoder_input , decoder_target\n",
    "\n",
    "encoder_input = tf.placeholder(name='encoder_input',shape=[None,None],dtype=tf.int32)\n",
    "decoder_input = tf.placeholder(name='decoder_input',shape=[None,None],dtype=tf.int32)\n",
    "decoder_target= tf.placeholder(name='decoder_target',shape=[None,None],dtype=tf.int32)\n",
    "\n",
    "#ecoder input should be  [Max_time,batch]  time major  \n",
    "#decoder input should be [Max_time,batch]  time major\n",
    "#decoder_target should be [Max_time,batch] time major\n",
    "print(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T17:48:12.313901Z",
     "start_time": "2018-06-30T17:48:12.244815Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(?, ?, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#embedding for encoder , decoder\n",
    "\n",
    "\n",
    "#use tf.get_variable instead of tf.Variable\n",
    "encoder_embedding = tf.get_variable(name='encoder_embedding',\n",
    "                                    shape=[encoder_vocab_size,encoder_embedding_dim],\n",
    "                                    dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "\n",
    "decoder_embedding = tf.get_variable(name='decoder_embedding',\n",
    "                                    shape=[decoder_vocab_size,decoder_embedding_dim],\n",
    "                                    dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "\n",
    "embedd_encoder = tf.nn.embedding_lookup(encoder_embedding,encoder_input)\n",
    "#now encoder input will become [ max_time , batch_size , embedding_dim ]\n",
    "\n",
    "embedd_decoder = tf.nn.embedding_lookup(decoder_embedding,decoder_input)\n",
    "#now decoder input will become [ max_time , batch_size , embedding_dim ]\n",
    "\n",
    "print(embedd_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T17:48:14.252880Z",
     "start_time": "2018-06-30T17:48:14.246121Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cell for rnn\n",
    "\n",
    "encoder_cell = rnn.LSTMCell(num_units=encoder_hidden_unit)\n",
    "decoder_cell = rnn.LSTMCell(num_units=decoder_hidden_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T17:48:15.496305Z",
     "start_time": "2018-06-30T17:48:15.356068Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoder\n",
    "\n",
    "encoder_model, encoder_last_state = tf.nn.dynamic_rnn(cell=encoder_cell,\n",
    "                                                      inputs=embedd_encoder,\n",
    "                                                      time_major=True,\n",
    "                                                      dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T17:48:16.736098Z",
     "start_time": "2018-06-30T17:48:16.580623Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#decoder \n",
    "decoder_output,decoder_last_state= tf.nn.dynamic_rnn(cell=decoder_cell,\n",
    "                                                     inputs=embedd_decoder,\n",
    "                                                     time_major=True,\n",
    "                                                     initial_state=encoder_last_state,\n",
    "                                                     dtype=tf.float32,\n",
    "                                                     scope='decoder_inputs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T17:48:17.849716Z",
     "start_time": "2018-06-30T17:48:17.766936Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#projection layer without activation\n",
    "\n",
    "linear_projection = tf.contrib.layers.fully_connected(decoder_output,decoder_vocab_size)\n",
    "\n",
    "\n",
    "#taking max argument\n",
    "prediction = tf.argmax(linear_projection,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T17:48:18.972186Z",
     "start_time": "2018-06-30T17:48:18.923253Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(decoder_target,\n",
    "                                                     depth=decoder_vocab_size,dtype=tf.float32),\n",
    "                                                     logits=linear_projection)\n",
    "\n",
    "#reduce_mean\n",
    "loss=tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#our aim is to minimize this loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T17:35:05.380347Z",
     "start_time": "2018-06-30T17:35:04.607100Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T17:48:32.405781Z",
     "start_time": "2018-06-30T17:48:32.298745Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input \n",
      " [[6 5 9]\n",
      " [9 6 8]\n",
      " [9 8 7]\n",
      " [5 5 7]\n",
      " [3 1 5]]\n",
      "decoder_input \n",
      " [[9 2 3]\n",
      " [1 7 2]\n",
      " [5 8 3]\n",
      " [7 4 8]]\n",
      "prediction_data \n",
      " [[ 80 137 117]\n",
      " [ 80  74   4]\n",
      " [  3  65   4]\n",
      " [103  10  51]]\n"
     ]
    }
   ],
   "source": [
    "#let's check network\n",
    "\n",
    "#make sure your encoder and decoder input and decoder taget batch size is same , time_stamps can be differ but batch_size\n",
    "#should be same otherwise you will get dim mismatch Error\n",
    "\n",
    "#fake data\n",
    "encoder_input_ = np.random.randint(0,10,[5,3])  #time major  [max_time , batch]\n",
    "print(\"encoder_input \\n\",encoder_input_)\n",
    "\n",
    "decoder_input_ = np.random.randint(0,10,[4,3])  #time_major  [max_time , batch]\n",
    "print(\"decoder_input \\n\",decoder_input_)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    prediction_data = sess.run(prediction,feed_dict={encoder_input:encoder_input_,decoder_input:decoder_input_})\n",
    "    print(\"prediction_data \\n\",prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
