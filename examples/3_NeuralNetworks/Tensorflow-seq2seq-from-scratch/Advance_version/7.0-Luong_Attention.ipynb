{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Luong_Attention.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "mfRstwjv55mA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AlP_UDLf6KSX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Luong_ Attention argumants are . ==> rnn_size (num_units of lstm/gru cell) , \n",
        "\n",
        "# encoder_output [batch x max_time x dim] \n",
        "\n",
        "# sentence_length ( max _length of sequence in batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0GpWlj46tfa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#let's define encoder first \n",
        "\n",
        "batch_size = 4\n",
        "max_time   = 5\n",
        "dim        = 12\n",
        "num_units  = 6\n",
        "sequence_length = [2,4,5,1]\n",
        "\n",
        "\n",
        "input_data = np.random.randn(batch_size , max_time , dim).astype(np.float32)\n",
        "\n",
        "lstm_cell = tf.contrib.rnn.LSTMCell(num_units)\n",
        "\n",
        "output , last_state = tf.nn.dynamic_rnn(lstm_cell,input_data,sequence_length,dtype=tf.float32)\n",
        "\n",
        "#input batch x max x dim ==> lstm ==> (batch x max x lstm_dim , batch x lstm_dim )\n",
        "# return (4, 5, 6) , (4, 6)\n",
        "\n",
        "\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "#   print(sess.run(last_state[0]).shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4G1wM9Gk9-E4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#now let's Construct the Attention mechanism.\n",
        "\n",
        "\n",
        "Luong_Attention = tf.contrib.seq2seq.LuongAttention(\n",
        "                                                  num_units=num_units,\n",
        "                                                  memory=output,\n",
        "                                                  memory_sequence_length=sequence_length,\n",
        "                                                  dtype=None, \n",
        "                                                  name='LuongAttention'\n",
        "                                                 )\n",
        "\n",
        "#You can test and use it's various methods \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# will return [batch_size x alignments_size  ] for next time_stamp\n",
        "\n",
        "# [ batch_size, alignments_size] (alignments_size is memory's max_time).\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JdovMup1_txu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#let's print the alignment output \n",
        "\n",
        "#for that we need two variable , state and query\n",
        "\n",
        "query_ = tf.get_variable(name='query_dta_1',\n",
        "                         shape=[batch_size,num_units],\n",
        "                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
        "\n",
        "state_ = tf.get_variable(name='state__dta_l',\n",
        "                         shape=[batch_size,max_time],\n",
        "                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zg_OgD1HAQ-o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b869f08a-0d6b-481f-fd48-ee4f8d24e585",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530807209951,
          "user_tz": -330,
          "elapsed": 1237,
          "user": {
            "displayName": "ayodhyankit paul",
            "photoUrl": "//lh3.googleusercontent.com/-aLSMOExWjxQ/AAAAAAAAAAI/AAAAAAAAAAc/yPMgEhPgnpk/s50-c-k-no/photo.jpg",
            "userId": "106815194044651409765"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#initial_alignments\n",
        "initial_alignments = Luong_Attention.initial_alignments(batch_size,dtype=tf.float32)\n",
        "\n",
        "\n",
        "#initial_state\n",
        "initial_state   =    Luong_Attention.initial_state(batch_size,dtype=tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "call = Luong_Attention.__call__(query_,state_)\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  alignment , next_state = sess.run(call)\n",
        "  \n",
        "  print(alignment.shape,next_state.shape)\n",
        "  \n",
        "  #batch_size x max_time  alignment _shape\n",
        "  #batch_size x max_time  next_state_sgape\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 5) (4, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7XYdbBunAyYW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Now we don't need to do those things , Tensorflow have a wrapper called AttentionWrapper which will do all those things\n",
        "\n",
        "#attention_wrapper arguments ==> cell , attention_mech , rnn_size \n",
        "\n",
        "attention_wrapper = tf.contrib.seq2seq.AttentionWrapper(lstm_cell,\n",
        "                                                        Luong_Attention,\n",
        "                                                        num_units)\n",
        "\n",
        "#this will return \n",
        "\n",
        "#AttentionWrapperState   batch x num_units\n",
        "# attention vector       batch x num_units\n",
        "#current time_stamp      1 , 0 depend of time_staps\n",
        "#alignments              batch x max_time\n",
        "# alignment_history\n",
        "#attention_state         batch x max_time\n",
        "\n",
        "#     collections.namedtuple(\"AttentionWrapperState\",\n",
        "#                            (\"cell_state\", \"attention\", \n",
        "#                                \"time\", \"alignments\",\n",
        "#                             \"alignment_history\", \"attention_state\")))\n",
        "\n",
        "#   \"\"\"`namedtuple` storing the state of a `AttentionWrapper`.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HM4hU8_sDZOZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "4dbe9c58-713c-4353-fc12-05b4f704c36f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530807225433,
          "user_tz": -330,
          "elapsed": 920,
          "user": {
            "displayName": "ayodhyankit paul",
            "photoUrl": "//lh3.googleusercontent.com/-aLSMOExWjxQ/AAAAAAAAAAI/AAAAAAAAAAc/yPMgEhPgnpk/s50-c-k-no/photo.jpg",
            "userId": "106815194044651409765"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#let's check and unroll for zero state with timestamp 1\n",
        "\n",
        "# first we have to define zero_state\n",
        "\n",
        "zero_s= attention_wrapper.zero_state(batch_size=batch_size,dtype=tf.float32)\n",
        "\n",
        "wrapper_call = attention_wrapper.__call__(query_,zero_s)\n",
        "AttentionWrapperState , (cell_state , attention , time , alignments , alignment_history , attention_state) = wrapper_call\n",
        "\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  AttentionWrapperState , (cell_state , attention , time , alignments , alignment_history , attention_state) =sess.run(wrapper_call)\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  print( 'Attention_wrapper_state {} \\n '.format(AttentionWrapperState.shape))\n",
        "  print( 'cell_state {} \\n '.format(cell_state))\n",
        "  print('attention {} \\n '.format(attention.shape))\n",
        "  print('time {} \\n ',time)\n",
        "  print('alignments {} \\n '.format(alignments.shape))\n",
        "  print('alignment_history {} \\n ',alignment_history)\n",
        "  print('attention_state {} \\n ',attention_state.shape)\n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "#  Contains:\n",
        "#     - `cell_state`: The state of the wrapped `RNNCell` at the previous time\n",
        "#       step.\n",
        "#     - `attention`: The attention emitted at the previous time step.\n",
        "#     - `time`: int32 scalar containing the current time step.\n",
        "#     - `alignments`: A single or tuple of `Tensor`(s) containing the alignments\n",
        "#        emitted at the previous time step for each attention mechanism.\n",
        "#     - `alignment_history`: (if enabled) a single or tuple of `TensorArray`(s)\n",
        "#        containing alignment matrices from all time steps for each attention\n",
        "#        mechanism. Call `stack()` on each to convert to a `Tensor`.\n",
        "#     - `attention_state`: A single or tuple of nested objects\n",
        "#        containing attention mechanism state for each attention mechanism.\n",
        "#        The objects may contain Tensors or TensorArrays.\n",
        "#   \"\"\"\n",
        "\n"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention_wrapper_state (4, 6) \n",
            " \n",
            "cell_state LSTMStateTuple(c=array([[ 3.08043323e-03,  1.55311858e-03,  1.70145626e-03,\n",
            "         2.11932187e-04,  8.50574463e-04,  2.05059652e-03],\n",
            "       [-4.74626489e-04,  9.16106452e-04,  1.78456702e-03,\n",
            "        -1.44209852e-03,  7.57550995e-04, -1.64095720e-03],\n",
            "       [-3.85492458e-04, -1.17246434e-03,  1.75134250e-04,\n",
            "        -1.00763515e-04, -4.77696769e-04, -5.45556890e-04],\n",
            "       [ 2.44189054e-03,  2.35536245e-05,  1.76277978e-03,\n",
            "         3.67806439e-04,  3.43983644e-04,  1.76626770e-03]], dtype=float32), h=array([[ 1.54447532e-03,  7.77840789e-04,  8.49683536e-04,\n",
            "         1.06092135e-04,  4.24564350e-04,  1.02856825e-03],\n",
            "       [-2.36995256e-04,  4.58025868e-04,  8.91984731e-04,\n",
            "        -7.19621603e-04,  3.78311321e-04, -8.19765264e-04],\n",
            "       [-1.92408625e-04, -5.86739799e-04,  8.76856939e-05,\n",
            "        -5.03981573e-05, -2.39042798e-04, -2.72577658e-04],\n",
            "       [ 1.22316589e-03,  1.17831314e-05,  8.81763175e-04,\n",
            "         1.84339791e-04,  1.71725464e-04,  8.85150221e-04]], dtype=float32)) \n",
            " \n",
            "attention (4, 6) \n",
            " \n",
            "time {} \n",
            "  1\n",
            "alignments (4, 5) \n",
            " \n",
            "alignment_history {} \n",
            "  ()\n",
            "attention_state {} \n",
            "  (4, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YjHfyvq8FSwe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "4505d95a-4a8c-4d2d-a35b-25735a58a3f9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530807228499,
          "user_tz": -330,
          "elapsed": 1124,
          "user": {
            "displayName": "ayodhyankit paul",
            "photoUrl": "//lh3.googleusercontent.com/-aLSMOExWjxQ/AAAAAAAAAAI/AAAAAAAAAAc/yPMgEhPgnpk/s50-c-k-no/photo.jpg",
            "userId": "106815194044651409765"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#let's feed this time step for next one \n",
        "\n",
        "next_unroll = attention_wrapper.__call__(query_,wrapper_call[1])\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  print(sess.run(next_unroll))\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[ 0.15419605,  0.02215088,  0.09260561,  0.05548464,  0.03287077,\n",
            "        -0.24773316],\n",
            "       [ 0.17521137,  0.05044638, -0.15758426, -0.02293995,  0.07392441,\n",
            "        -0.2799775 ],\n",
            "       [-0.00876762,  0.12460517,  0.0767331 , -0.04451225, -0.00721595,\n",
            "         0.02627574],\n",
            "       [ 0.00121187, -0.13014013,  0.0483407 ,  0.12857303,  0.05783688,\n",
            "        -0.12511724]], dtype=float32), AttentionWrapperState(cell_state=LSTMStateTuple(c=array([[-0.0517728 , -0.07100058,  0.04009656, -0.01267772,  0.01637715,\n",
            "         0.01745801],\n",
            "       [-0.03136916, -0.05405451,  0.01539867, -0.01264702,  0.06239711,\n",
            "         0.03233024],\n",
            "       [-0.01143507,  0.01212406, -0.02517536,  0.02526897, -0.02043939,\n",
            "        -0.01568646],\n",
            "       [-0.04043157, -0.03508674,  0.07172593, -0.03577641,  0.02680731,\n",
            "         0.02483258]], dtype=float32), h=array([[-0.02548631, -0.03585832,  0.02078494, -0.00657643,  0.00795086,\n",
            "         0.00852645],\n",
            "       [-0.01629876, -0.02728193,  0.00779214, -0.00644129,  0.03004474,\n",
            "         0.01576882],\n",
            "       [-0.00573416,  0.00598284, -0.01281709,  0.01293437, -0.0103511 ,\n",
            "        -0.00790804],\n",
            "       [-0.01982848, -0.01795979,  0.03605331, -0.01772337,  0.01287513,\n",
            "         0.01215158]], dtype=float32)), attention=array([[ 0.15419605,  0.02215088,  0.09260561,  0.05548464,  0.03287077,\n",
            "        -0.24773316],\n",
            "       [ 0.17521137,  0.05044638, -0.15758426, -0.02293995,  0.07392441,\n",
            "        -0.2799775 ],\n",
            "       [-0.00876762,  0.12460517,  0.0767331 , -0.04451225, -0.00721595,\n",
            "         0.02627574],\n",
            "       [ 0.00121187, -0.13014013,  0.0483407 ,  0.12857303,  0.05783688,\n",
            "        -0.12511724]], dtype=float32), time=2, alignments=array([[0.4988134 , 0.5011866 , 0.        , 0.        , 0.        ],\n",
            "       [0.24770409, 0.25034893, 0.25059932, 0.25134757, 0.        ],\n",
            "       [0.19988278, 0.19986606, 0.20005208, 0.20036341, 0.19983569],\n",
            "       [1.        , 0.        , 0.        , 0.        , 0.        ]],\n",
            "      dtype=float32), alignment_history=(), attention_state=array([[0.4988134 , 0.5011866 , 0.        , 0.        , 0.        ],\n",
            "       [0.24770409, 0.25034893, 0.25059932, 0.25134757, 0.        ],\n",
            "       [0.19988278, 0.19986606, 0.20005208, 0.20036341, 0.19983569],\n",
            "       [1.        , 0.        , 0.        , 0.        , 0.        ]],\n",
            "      dtype=float32)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iA8nYpPJIxqn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eWAx3lbCOTE1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
