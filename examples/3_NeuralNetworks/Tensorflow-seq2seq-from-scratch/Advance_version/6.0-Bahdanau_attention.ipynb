{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bahdanau_attention.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "mfRstwjv55mA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AlP_UDLf6KSX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Bahdanau_attention argumants are . ==> rnn_size (num_units of lstm/gru cell) , \n",
        "\n",
        "# encoder_output [batch x max_time x dim] \n",
        "\n",
        "# sentence_length ( max _length of sequence in batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0GpWlj46tfa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#let's define encoder first \n",
        "\n",
        "batch_size = 4\n",
        "max_time   = 5\n",
        "dim        = 12\n",
        "num_units  = 6\n",
        "sequence_length = [2,4,5,1]\n",
        "\n",
        "\n",
        "input_data = np.random.randn(batch_size , max_time , dim).astype(np.float32)\n",
        "\n",
        "lstm_cell = tf.contrib.rnn.LSTMCell(num_units)\n",
        "\n",
        "output , last_state = tf.nn.dynamic_rnn(lstm_cell,input_data,sequence_length,dtype=tf.float32)\n",
        "\n",
        "#input batch x max x dim ==> lstm ==> (batch x max x lstm_dim , batch x lstm_dim )\n",
        "# return (4, 5, 6) , (4, 6)\n",
        "\n",
        "\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "#   print(sess.run(last_state[0]).shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4G1wM9Gk9-E4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#now let's Construct the Attention mechanism.\n",
        "\n",
        "\n",
        "attention_Bahdanau = tf.contrib.seq2seq.BahdanauAttention(\n",
        "                                                  num_units=num_units,\n",
        "                                                  memory=output,\n",
        "                                                  memory_sequence_length=sequence_length,\n",
        "                                                  dtype=None, \n",
        "                                                  name='BahdanauAttention'\n",
        "                                                 )\n",
        "\n",
        "#You can test and use it's various methods \n",
        "\n",
        "# batch_size = attention_Bahdanau.batch_size\n",
        "# memory_layer = attention_Bahdanau.memory_layer\n",
        "# keys=attention_Bahdanau.keys\n",
        "# values=attention_Bahdanau.values\n",
        "# alignment_size= attention_Bahdanau.alignments_size\n",
        "# state_size = attention_Bahdanau.state_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# will return [batch_size x alignments_size  ] for next time_stamp\n",
        "\n",
        "# [ batch_size, alignments_size] (alignments_size is memory's max_time).\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JdovMup1_txu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#let's print the alignment output \n",
        "\n",
        "#for that we need two variable , state and query\n",
        "\n",
        "query_ = tf.get_variable(name='query_dta_1',\n",
        "                         shape=[batch_size,num_units],\n",
        "                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
        "\n",
        "state_ = tf.get_variable(name='state__dta_l',\n",
        "                         shape=[batch_size,max_time],\n",
        "                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zg_OgD1HAQ-o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0afe6c5f-e104-45d8-e73f-8d421e15c04b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530806676710,
          "user_tz": -330,
          "elapsed": 963,
          "user": {
            "displayName": "ayodhyankit paul",
            "photoUrl": "//lh3.googleusercontent.com/-aLSMOExWjxQ/AAAAAAAAAAI/AAAAAAAAAAc/yPMgEhPgnpk/s50-c-k-no/photo.jpg",
            "userId": "106815194044651409765"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#initial_alignments\n",
        "initial_alignments = attention_Bahdanau.initial_alignments(batch_size,dtype=tf.float32)\n",
        "\n",
        "\n",
        "#initial_state\n",
        "initial_state   =    attention_Bahdanau.initial_state(batch_size,dtype=tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "call = attention_Bahdanau.__call__(query_,state_)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  alignment , next_state = sess.run(call)\n",
        "  \n",
        "  print(alignment.shape,next_state.shape)\n",
        "  \n",
        "  #batch_size x max_time  alignment _shape\n",
        "  #batch_size x max_time  next_state_sgape\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 5) (4, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7XYdbBunAyYW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Now we don't need to do those things , Tensorflow have a wrapper called AttentionWrapper which will do all those things\n",
        "\n",
        "#attention_wrapper arguments ==> cell , attention_mech , rnn_size \n",
        "\n",
        "attention_wrapper = tf.contrib.seq2seq.AttentionWrapper(lstm_cell,\n",
        "                                                        attention_Bahdanau,\n",
        "                                                        num_units)\n",
        "\n",
        "#this will return \n",
        "\n",
        "#AttentionWrapperState   batch x num_units\n",
        "# attention vector       batch x num_units\n",
        "#current time_stamp      1 , 0 depend of time_staps\n",
        "#alignments              batch x max_time\n",
        "# alignment_history\n",
        "#attention_state         batch x max_time\n",
        "\n",
        "#     collections.namedtuple(\"AttentionWrapperState\",\n",
        "#                            (\"cell_state\", \"attention\", \n",
        "#                                \"time\", \"alignments\",\n",
        "#                             \"alignment_history\", \"attention_state\")))\n",
        "\n",
        "#   \"\"\"`namedtuple` storing the state of a `AttentionWrapper`.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HM4hU8_sDZOZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "6abebe7d-4ecd-4e0e-f763-46e0ac8d63f6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530806682921,
          "user_tz": -330,
          "elapsed": 925,
          "user": {
            "displayName": "ayodhyankit paul",
            "photoUrl": "//lh3.googleusercontent.com/-aLSMOExWjxQ/AAAAAAAAAAI/AAAAAAAAAAc/yPMgEhPgnpk/s50-c-k-no/photo.jpg",
            "userId": "106815194044651409765"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#let's check and unroll for zero state with timestamp 1\n",
        "\n",
        "# first we have to define zero_state\n",
        "\n",
        "zero_s= attention_wrapper.zero_state(batch_size=batch_size,dtype=tf.float32)\n",
        "\n",
        "wrapper_call = attention_wrapper.__call__(query_,zero_s)\n",
        "AttentionWrapperState , (cell_state , attention , time , alignments , alignment_history , attention_state) = wrapper_call\n",
        "\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  AttentionWrapperState , (cell_state , attention , time , alignments , alignment_history , attention_state) =sess.run(wrapper_call)\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  print( 'Attention_wrapper_state {} \\n '.format(AttentionWrapperState.shape))\n",
        "  print( 'cell_state {} \\n '.format(cell_state))\n",
        "  print('attention {} \\n '.format(attention.shape))\n",
        "  print('time {} \\n ',time)\n",
        "  print('alignments {} \\n '.format(alignments.shape))\n",
        "  print('alignment_history {} \\n ',alignment_history)\n",
        "  print('attention_state {} \\n ',attention_state.shape)\n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "#  Contains:\n",
        "#     - `cell_state`: The state of the wrapped `RNNCell` at the previous time\n",
        "#       step.\n",
        "#     - `attention`: The attention emitted at the previous time step.\n",
        "#     - `time`: int32 scalar containing the current time step.\n",
        "#     - `alignments`: A single or tuple of `Tensor`(s) containing the alignments\n",
        "#        emitted at the previous time step for each attention mechanism.\n",
        "#     - `alignment_history`: (if enabled) a single or tuple of `TensorArray`(s)\n",
        "#        containing alignment matrices from all time steps for each attention\n",
        "#        mechanism. Call `stack()` on each to convert to a `Tensor`.\n",
        "#     - `attention_state`: A single or tuple of nested objects\n",
        "#        containing attention mechanism state for each attention mechanism.\n",
        "#        The objects may contain Tensors or TensorArrays.\n",
        "#   \"\"\"\n",
        "\n"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention_wrapper_state (4, 6) \n",
            " \n",
            "cell_state LSTMStateTuple(c=array([[ 1.5111677e-03,  4.0882453e-04,  1.3467169e-03,  1.7782598e-03,\n",
            "        -1.2312753e-03,  1.5057663e-03],\n",
            "       [ 2.3543267e-03, -1.9105258e-03,  1.1930702e-03,  7.3114608e-04,\n",
            "         4.3127441e-04,  1.0493306e-05],\n",
            "       [-1.5790670e-03, -6.5457175e-04, -3.7221948e-04, -8.0193178e-04,\n",
            "         5.4796168e-04, -1.5430629e-03],\n",
            "       [-1.6524252e-03,  3.0396411e-03,  9.2920812e-04,  5.6705176e-04,\n",
            "        -2.6813590e-03,  3.5907242e-03]], dtype=float32), h=array([[ 7.5694249e-04,  2.0503084e-04,  6.7268399e-04,  8.8943960e-04,\n",
            "        -6.1760383e-04,  7.5344433e-04],\n",
            "       [ 1.1791448e-03, -9.5691084e-04,  5.9497467e-04,  3.6539393e-04,\n",
            "         2.1601986e-04,  5.2492933e-06],\n",
            "       [-7.8811386e-04, -3.2676206e-04, -1.8669791e-04, -4.0103088e-04,\n",
            "         2.7398436e-04, -7.7201752e-04],\n",
            "       [-8.2568289e-04,  1.5217348e-03,  4.6479961e-04,  2.8396057e-04,\n",
            "        -1.3408703e-03,  1.7910090e-03]], dtype=float32)) \n",
            " \n",
            "attention (4, 6) \n",
            " \n",
            "time {} \n",
            "  1\n",
            "alignments (4, 5) \n",
            " \n",
            "alignment_history {} \n",
            "  ()\n",
            "attention_state {} \n",
            "  (4, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YjHfyvq8FSwe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "c2209f76-5fd2-4a7c-885e-a0b0b4bf17fd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530806706633,
          "user_tz": -330,
          "elapsed": 1448,
          "user": {
            "displayName": "ayodhyankit paul",
            "photoUrl": "//lh3.googleusercontent.com/-aLSMOExWjxQ/AAAAAAAAAAI/AAAAAAAAAAc/yPMgEhPgnpk/s50-c-k-no/photo.jpg",
            "userId": "106815194044651409765"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#let's feed this time step for next one \n",
        "\n",
        "next_unroll = attention_wrapper.__call__(query_,wrapper_call[1])\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  print(sess.run(next_unroll))\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[ 0.02850083, -0.08838632,  0.03423838, -0.04875825,  0.02433623,\n",
            "         0.01494354],\n",
            "       [ 0.0340319 ,  0.14170066, -0.03641218,  0.06206513, -0.02657704,\n",
            "        -0.15244852],\n",
            "       [ 0.05387848, -0.05067005, -0.02691523,  0.00859693,  0.07006565,\n",
            "         0.07642467],\n",
            "       [ 0.08855353,  0.12874618, -0.12630436,  0.0099225 , -0.07797796,\n",
            "         0.03092834]], dtype=float32), AttentionWrapperState(cell_state=LSTMStateTuple(c=array([[ 0.00654604, -0.01203434, -0.00615299,  0.0005315 , -0.00387453,\n",
            "        -0.00444581],\n",
            "       [ 0.01928671, -0.0066505 ,  0.01987001,  0.01775809,  0.0203748 ,\n",
            "         0.02813004],\n",
            "       [-0.00456914,  0.00182934, -0.00227972, -0.02489319,  0.00236064,\n",
            "        -0.02902747],\n",
            "       [-0.00715777,  0.00372725,  0.01859315, -0.00267803, -0.01072733,\n",
            "        -0.00648396]], dtype=float32), h=array([[ 0.00330853, -0.00600193, -0.00309194,  0.00027005, -0.00192373,\n",
            "        -0.00221843],\n",
            "       [ 0.00961715, -0.00332273,  0.01001352,  0.00857936,  0.01010879,\n",
            "         0.01378607],\n",
            "       [-0.00225938,  0.00092878, -0.00112528, -0.01261584,  0.00117257,\n",
            "        -0.01440058],\n",
            "       [-0.00357611,  0.00189408,  0.00896786, -0.00130601, -0.0052112 ,\n",
            "        -0.00312695]], dtype=float32)), attention=array([[ 0.02850083, -0.08838632,  0.03423838, -0.04875825,  0.02433623,\n",
            "         0.01494354],\n",
            "       [ 0.0340319 ,  0.14170066, -0.03641218,  0.06206513, -0.02657704,\n",
            "        -0.15244852],\n",
            "       [ 0.05387848, -0.05067005, -0.02691523,  0.00859693,  0.07006565,\n",
            "         0.07642467],\n",
            "       [ 0.08855353,  0.12874618, -0.12630436,  0.0099225 , -0.07797796,\n",
            "         0.03092834]], dtype=float32), time=2, alignments=array([[0.5233706 , 0.4766294 , 0.        , 0.        , 0.        ],\n",
            "       [0.3698612 , 0.285511  , 0.15458336, 0.1900444 , 0.        ],\n",
            "       [0.17603096, 0.25858277, 0.15518893, 0.22356178, 0.1866356 ],\n",
            "       [1.        , 0.        , 0.        , 0.        , 0.        ]],\n",
            "      dtype=float32), alignment_history=(), attention_state=array([[0.5233706 , 0.4766294 , 0.        , 0.        , 0.        ],\n",
            "       [0.3698612 , 0.285511  , 0.15458336, 0.1900444 , 0.        ],\n",
            "       [0.17603096, 0.25858277, 0.15518893, 0.22356178, 0.1866356 ],\n",
            "       [1.        , 0.        , 0.        , 0.        , 0.        ]],\n",
            "      dtype=float32)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iA8nYpPJIxqn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
