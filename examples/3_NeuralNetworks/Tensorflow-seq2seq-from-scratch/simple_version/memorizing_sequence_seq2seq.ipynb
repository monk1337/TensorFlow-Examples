{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:50:13.788603Z",
     "start_time": "2018-06-30T18:50:13.783803Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "#reference : https://arxiv.org/pdf/1409.3215.pdf\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import helpers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:50:14.827080Z",
     "start_time": "2018-06-30T18:50:14.817786Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some hyper parameters\n",
    "\n",
    "encoder_vocab_size = 100  # suppose our encoding data vocab size is 100 \n",
    "decoder_vocab_size = 150  # and decoding data vocab size is 150\n",
    "\n",
    "batch_size=10\n",
    "\n",
    "encoder_hidden_unit = 100\n",
    "decoder_hidden_unit = 100\n",
    "\n",
    "encoder_embedding_dim=50\n",
    "decoder_embedding_dim=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:50:15.926532Z",
     "start_time": "2018-06-30T18:50:15.913006Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"encoder_input:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#inputs :  encoder_input , decoder_input , decoder_target\n",
    "\n",
    "\n",
    "encoder_input = tf.placeholder(name='encoder_input',shape=[None,None],dtype=tf.int32)\n",
    "decoder_input = tf.placeholder(name='decoder_input',shape=[None,None],dtype=tf.int32)\n",
    "decoder_target= tf.placeholder(name='decoder_target',shape=[None,None],dtype=tf.int32)\n",
    "\n",
    "#ecoder input should be  [Max_time,batch]  time major  \n",
    "#decoder input should be [Max_time,batch]  time major\n",
    "#decoder_target should be [Max_time,batch] time major\n",
    "print(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:50:17.108003Z",
     "start_time": "2018-06-30T18:50:17.077151Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(?, ?, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#embedding for encoder , decoder\n",
    "\n",
    "\n",
    "\n",
    "#use tf.get_variable instead of tf.Variable\n",
    "encoder_embedding = tf.get_variable(name='encoder_embedding',\n",
    "                                    shape=[encoder_vocab_size,encoder_embedding_dim],\n",
    "                                    dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "\n",
    "decoder_embedding = tf.get_variable(name='decoder_embedding',\n",
    "                                    shape=[decoder_vocab_size,decoder_embedding_dim],\n",
    "                                    dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "\n",
    "embedd_encoder = tf.nn.embedding_lookup(encoder_embedding,encoder_input)\n",
    "#now encoder input will become [ max_time , batch_size , embedding_dim ]\n",
    "\n",
    "embedd_decoder = tf.nn.embedding_lookup(decoder_embedding,decoder_input)\n",
    "#now decoder input will become [ max_time , batch_size , embedding_dim ]\n",
    "\n",
    "print(embedd_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:50:19.211425Z",
     "start_time": "2018-06-30T18:50:19.205812Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cell for rnn\n",
    "\n",
    "encoder_cell = rnn.LSTMCell(num_units=encoder_hidden_unit)\n",
    "decoder_cell = rnn.LSTMCell(num_units=decoder_hidden_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:50:20.406582Z",
     "start_time": "2018-06-30T18:50:20.266266Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoder\n",
    "\n",
    "encoder_model, encoder_last_state = tf.nn.dynamic_rnn(cell=encoder_cell,\n",
    "                                                      inputs=embedd_encoder,\n",
    "                                                      time_major=True,\n",
    "                                                      dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:50:22.149576Z",
     "start_time": "2018-06-30T18:50:22.021865Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#decoder \n",
    "decoder_output,decoder_last_state= tf.nn.dynamic_rnn(cell=decoder_cell,\n",
    "                                                     inputs=embedd_decoder,\n",
    "                                                     time_major=True,\n",
    "                                                     initial_state=encoder_last_state,\n",
    "                                                     dtype=tf.float32,\n",
    "                                                     scope='decoder_inputs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:50:23.328495Z",
     "start_time": "2018-06-30T18:50:23.250940Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#projection layer without activation\n",
    "\n",
    "linear_projection = tf.contrib.layers.fully_connected(decoder_output,decoder_vocab_size)\n",
    "\n",
    "\n",
    "#taking max argument\n",
    "prediction = tf.argmax(linear_projection,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:50:25.414696Z",
     "start_time": "2018-06-30T18:50:25.366434Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(decoder_target,\n",
    "                                                     depth=decoder_vocab_size,dtype=tf.float32),\n",
    "                                                     logits=linear_projection)\n",
    "\n",
    "#reduce_mean\n",
    "loss=tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#our aim is to minimize this loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:50:28.061567Z",
     "start_time": "2018-06-30T18:50:27.385508Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:47:44.717641Z",
     "start_time": "2018-06-30T18:47:44.486022Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input \n",
      " [[7 2 3]\n",
      " [4 4 2]\n",
      " [3 1 1]\n",
      " [3 0 4]\n",
      " [1 5 1]]\n",
      "decoder_input \n",
      " [[7 2 6]\n",
      " [4 6 6]\n",
      " [5 3 0]\n",
      " [8 4 9]]\n",
      "prediction_data \n",
      " [[ 45  50  15]\n",
      " [111  50  15]\n",
      " [111  50  17]\n",
      " [  6 111  15]]\n"
     ]
    }
   ],
   "source": [
    "# #let's check network\n",
    "\n",
    "# #make sure your encoder and decoder input and decoder taget batch size is same , time_stamps can be differ but batch_size\n",
    "# #should be same otherwise you will get dim mismatch Error\n",
    "\n",
    "# #fake data\n",
    "# encoder_input_ = np.random.randint(0,10,[5,3])  #time major  [max_time , batch]\n",
    "# print(\"encoder_input \\n\",encoder_input_)\n",
    "\n",
    "# decoder_input_ = np.random.randint(0,10,[4,3])  #time_major  [max_time , batch]\n",
    "# print(\"decoder_input \\n\",decoder_input_)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     prediction_data = sess.run(prediction,feed_dict={encoder_input:encoder_input_,decoder_input:decoder_input_})\n",
    "#     print(\"prediction_data \\n\",prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:53:50.732378Z",
     "start_time": "2018-06-30T18:50:55.972610Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 5.005568504333496\n",
      "  sample 1:\n",
      "    input     > [7 9 6 8 4 0 0 0]\n",
      "    predicted > [6 6 5 5 5 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 2 8 0 0 0 0 0]\n",
      "    predicted > [6 6 6 5 5 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 4 2 6 9 8 3 0]\n",
      "    predicted > [6 6 6 6 6 5 5 5 5]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 1.1616154909133911\n",
      "  sample 1:\n",
      "    input     > [6 3 7 8 2 7 9 0]\n",
      "    predicted > [7 7 7 7 7 7 7 1 0]\n",
      "  sample 2:\n",
      "    input     > [2 3 7 9 9 0 0 0]\n",
      "    predicted > [7 7 7 7 7 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 5 3 0 0 0 0 0]\n",
      "    predicted > [2 4 3 1 0 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.19712510704994202\n",
      "  sample 1:\n",
      "    input     > [5 2 4 8 0 0 0 0]\n",
      "    predicted > [5 2 4 8 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 8 7 9 9 2 2 5]\n",
      "    predicted > [8 8 7 9 8 2 2 5 1]\n",
      "  sample 3:\n",
      "    input     > [4 6 5 8 9 5 0 0]\n",
      "    predicted > [4 6 9 8 9 5 1 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.059102702885866165\n",
      "  sample 1:\n",
      "    input     > [7 2 6 3 5 5 9 0]\n",
      "    predicted > [7 2 6 3 5 5 9 1 0]\n",
      "  sample 2:\n",
      "    input     > [5 9 3 2 0 0 0 0]\n",
      "    predicted > [5 9 3 2 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 4 8 6 6 7 9 3]\n",
      "    predicted > [5 4 8 6 6 7 9 3 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, _ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in batch]\n",
    "    )\n",
    "    decoder_inputs_, _ = helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_input: encoder_inputs_,\n",
    "        decoder_input: decoder_inputs_,\n",
    "        decoder_target: decoder_targets_,\n",
    "    }\n",
    "\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "loss_track = []\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_input].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T18:54:04.312640Z",
     "start_time": "2018-06-30T18:54:02.450025Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0657 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEgJAjoApMT/AwL9K69QHArmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd232802b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
